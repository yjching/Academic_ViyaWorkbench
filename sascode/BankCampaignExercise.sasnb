[{"kind":1,"language":"markdown","value":"# SAS Hands-On Workshop Exercise\n---\n\nFor this exercise, your goal is to debug the code below.  Run the code from start to finish, then locate and correct the three errors that prevent it from running cleanly. \n\n_______________________________________","outputs":[]},{"kind":1,"language":"markdown","value":"System Options","outputs":[]},{"kind":2,"language":"sas","value":"* options source source2 mprint mlogic symbolgen;\n* options nosource nosource2 nomprint nomlogic nosymbolgen;\nods graphics on;","outputs":[]},{"kind":1,"language":"markdown","value":"### **Step 1: LOAD the input file**","outputs":[]},{"kind":2,"language":"sas","value":"proc import datafile = \"/workspaces/myfolder/CBR_ViyaWorkbench/data/bank.csv\"\n            out = bank dbms = csv replace;\n            guessingrows=30;\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"### **Step 2: EXPLORE the data**\n\nExploratory data analysis (EDA) is crucial in data science projects because it helps us understand the structure and characteristics of the data we're working with. By exploring variables, identifying patterns, detecting anomalies, and visualizing relationships, EDA enables us to make informed decisions about data preprocessing, feature engineering, and model selection. It also plays a key role in uncovering insights and formulating hypotheses, laying the groundwork for more accurate modeling and impactful conclusions.","outputs":[]},{"kind":1,"language":"markdown","value":"Review the proc CONTENTS, proc MEAN, and print 10 observations.\n\n* From proc CONTENT, we note that Activity_Status and Customer_Value are Char types when we know they are categorical variables. Hence, we should consider to encode these variables.\n* From proc MEANS, we note that there are missing data in Age and AvgSale3Yr_DP.","outputs":[]},{"kind":2,"language":"sas","value":"ods select Variables;\nproc contents data=bank;\nrun;\nods select default;\n\ntitle \"Summary Statistics of dataset\";\nproc means data = bank n nmiss mean min max std;\n  ods exclude sortinfo;\nrun;\n\ntitle \"First 10 rows of dataset\";\nproc print data=bank (obs=10);\nrun;\ntitle;","outputs":[]},{"kind":1,"language":"markdown","value":"Graph the distribution of the target variable\n\n* From the distribution, we note that the data is heavily imbalanced.","outputs":[]},{"kind":2,"language":"sas","value":"proc freq data=bank;\n    tables Status / out=freq_status;\n    title \"Frequency distribution of target variable\";\nrun;\n\nproc sgplot data=freq_status;\n    vbar Status / response=Count stat=sum;\n    xaxis label='Status';\n    yaxis label='Frequency';\n    title 'Frequency Distribution of Status';\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"Additional observations:\n\n* Most customers wait 15-21 months before purchasing again.\n* We find that the cutsomer value groups help represent how these \ncutsomers contribute to the firm's sales. Those in the lowest value group are \nobserved to have the highest activity status.\n* We find that the cutsomer value groups help represent how these \ncutsomers contribute to the firm's sales. Those with high activity \n(buy more often), tend to spend less in their lifetime than those who have an \naverage or low activityty status.","outputs":[]},{"kind":2,"language":"sas","value":"proc freq data=bank noprint;\n    tables MnthsLastPur / out=freq_mlp;\nrun;\n\nproc sgplot data=freq_mlp;\n    vbar MnthsLastPur / response=Count stat=sum;\n    xaxis label='Months Since Last Purchase';\n    yaxis label='Frequency';\n    title 'Frequency Distribution of Months Since Last Purchase';\nrun;\n\nproc sgplot data=bank;\n    vbar Customer_Value /  \n        group=Activity_Status \n        nostatlabel\n        groupdisplay=cluster;\n    xaxis label='Customer Value';\n    yaxis label='Count';\n    title 'Customer Value Groups by Activity Status';\nrun;\n\nproc means data=bank noprint;\n    class Activity_Status;\n    var AvgSaleLife;\n    output out=sum_count\n        mean=TotalSum\n        n=Count;\nrun;\n\ndata avg_sale_life_per_customer;\n    set sum_count;\n    AvgSaleLifePerCustomer = TotalSum / Count;\nrun;\n\nproc sgplot data=avg_sale_life_per_customer;\n    vbar Activity_Status / response=AvgSaleLifePerCustomer;\n    xaxis label='Activity Status';\n    yaxis label='AvgSaleLife per Customer';\n    title 'Proportionate AvgSaleLife per Customer by Activity Status';\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"### **Step 3:  PRE-PROCESS the data**\n\nData wrangling is essential in the model creation cycle as it ensures data quality, prepares data for modeling techniques, uncovers insights, and supports reproducibility. It forms the foundation upon which accurate, reliable, and actionable models can be derived from data in the field of data science. These steps are done based on what our exploratory data analysis (EDA) uncovered. In this case, we will be imputing our missing variables, encoding our categorical variables, and splitting our dataset for testing and training.","outputs":[]},{"kind":1,"language":"markdown","value":"#### Imputation\n\nEarlier in this project we saw that our data was missing about 25% of data of Age and AvgSale3Yr_DP. To best support our model creation, we will address this by replacing our missing data by the means of the corresponding variable.","outputs":[]},{"kind":2,"language":"sas","value":"proc means data=bank noprint;\n   var Age AvgSale3Yr_DP;\n   output out=means_result mean=mean_Age mean_AvgSale3Yr_DP;\nrun;\n\ndata bank_modified;\n   set bank;\n   if _N_ = 1 then set means_result\n   \n   if missing(Age) then Age = mean_Age;\n   if missing(AvgSale3Yr_DP) then AvgSale3Yr_DP = mean_AvgSale3Yr_DP;\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"#### Label Encoding\n\nThis step will makes our categorical variables be accurately represented as numbers. We identidied that our categorical variables hold valueable insight into the purchasing habits, which means we aim to keep them in our models for accurate results.","outputs":[]},{"kind":2,"language":"sas","value":"proc format;\n    value $activityfmt\n        'High' = '1'\n        'Average' = '2'\n        'Low' = '3';\n        \n    value $custfmt\n        'A' = '1'\n        'B' = '2'\n        'C' = '3'\n        'D' = '4'\n        'E' = '5';\nrun;\n\ndata bank_modified;\n    set bank_modified;\n    format Activity_Status $activityfmt. Customer_Value $custfmt.;\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"#### Train/Test Split\n\nWe split our dataset into training and testing splits in order to ensure our model is being trained without knowing our target variable and having all the information. This is done so th emodel can be more robust to unseen data, which would be new customer data in our case. The testing spilt will then determine our model's performance as the predicted values of Status are then compared to actual results of Status.\n\nCreate a 60/30/10 split, save the hold out sample in a separate data set.","outputs":[]},{"kind":2,"language":"sas","value":"proc partition data=bank_modified partind samppct=60 samppct2=30;\n\tby Status;\n\toutput out=bankPART;\n\tods exclude OutputCasTables STRAFreq;\nrun;\n\ntitle 'Partitioned Dataset';\nproc freq data=bankPART;\ntable _PartIND_;\nrun;\ntitle;\n\ndata bankTEST(drop=_partind_);\n   set bankPART(where=(_partind_=0));\nrun;\n\ndata bankPART;\n   set bankPART(where=(_partind_ in (1,2)));\nrun;\n\nods noproctitle;\ntitle 'Training Data';\nproc freq data=bankPART;\n  table status;\nrun;\n\ntitle 'Hold Out Sample';\nproc freq data=bankTEST;\n   table status;\nrun;\ntitle;","outputs":[]},{"kind":1,"language":"markdown","value":"### **Step 4: TRAIN a model**\n\nModeling in the data science process involves the application of machine learning algorithms to analyze data, make predictions, or uncover patterns. It is a pivotal phase where the insights gleaned from data are translated into actionable decisions and solutions.\n\nMachine learning models are employed to address various tasks, such as classification, regression, clustering, and recommendation systems, depending on the nature of the problem at hand. These models learn from historical data to generalize patterns and make predictions on new, unseen data.\n\nFor this project, we will be highlighting models from 2 sources:\n* Python sklean Models\n* SAS Viya ML Models","outputs":[]},{"kind":1,"language":"markdown","value":"#### Random Forest\n\nThe example showed how we can perform SAS Viya analytic processes without a\n separate cloud-based server. It also demonstrated the use of saving analytic\n stores into files that can be used by any other product that supports them.\n As a result, a user can use SAS Viya Workbench to quickly try out ideas by\n building and testing models, which can then be saved for use in other\n environments as needed.","outputs":[]},{"kind":2,"language":"sas","value":"title 'Random Forest trained on partitioned data';\nods output FitStatistics=forestFitStatistics;\nproc forest data=bankPART ntrees=100 seed=42;\n    target Status / level=nominal;\n    partition role=_partind_(train='1' validate='2');\n    input Activity_Status Customer_Value Home_Flag / level=nominal;\n    input Age Homeval Inc Pr AvgSale3Yr AvgSaleLife\tAvgSale3Yr_DP LastProdAmt\n        CntPur3Yr CntPurLife CntPur3Yr_DP CntPurLife_DP\tCntTotPromo\tMnthsLastPur\n        Cnt1Yr_DP CustTenure / level=interval;\n    savestate rstore=forestAstore;\n    ods exclude outputCasTables;\nrun;\ntitle;","outputs":[]},{"kind":1,"language":"markdown","value":"RANDOM FOREST: Save the Analytic Store","outputs":[]},{"kind":2,"language":"sas","value":"proc astore;\n    download rstore=forestAstore store=\"/workspaces/myfolder/CBR_ViyaWorkbench/astores/WB_forest.sasast\";\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"#### Gradient Boosting\n\nThis example shows how to build a gradient boosing model using\n              the GRADBOOST procedure without the need to use a separate cloud-\n              based server. It also demonstrates the ability for the model to be\n              deployed to Model Manager on SAS Viya, ensuring enterprise level\n              deployment and model governance is achieved.","outputs":[]},{"kind":2,"language":"sas","value":"title 'Gradient Boost trained on partitioned data';\nods output FitStatistics=gboostFitStatistics; \nproc gradboost data=bankPART ntrees=100 seed=42;\n    target Status / level=nominal;\n    partition role=_partind_(train='1' validate='2');\n    input Activity_Status Customer_Value Home_Flag / level=nominal;\n    input Age Homeval Inc Pr AvgSale3Yr AvgSaleLife\tAvgSale3Yr_DP LastProdAmt\n        CntPur3Yr CntPurLife CntPur3Yr_DP CntPurLife_DP\tCntTotPromo\tMnthsLastPur\n        Cnt1Yr_DP CustTenure / level=interval;\n        id AccountID;\n    savestate rstore=gboostAstore;\n    ods exclude outputCasTables;\n ;\n title;","outputs":[]},{"kind":1,"language":"markdown","value":"GBOOST: Save the Analytic Store","outputs":[]},{"kind":2,"language":"sas","value":"proc astore;\n    download rstore=gboostAstore store=\"/workspaces/myfolder/CBR_ViyaWorkbench/astores/WB_gboost.sasast\";\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"### **Step 5: EVALUATE the results**\n\nModel evaluation is crucial in a data science project because it helps assess how well a model generalizes to new, unseen data. By evaluating a model, we can understand its accuracy, robustness, and potential limitations, ensuring that it performs as expected in real-world scenarios. It allows us to compare different models, choose the best-performing one, and identify areas for improvement, such as addressing overfitting or underfitting. Effective model evaluation ultimately ensures that the model delivers reliable and actionable insights, making it essential for decision-making and project success.","outputs":[]},{"kind":1,"language":"markdown","value":"RANDOM FOREST: Fit Statistics","outputs":[]},{"kind":2,"language":"sas","value":"data forestTRAINfitstats;\n  set FORESTfitstatistics3;\n  length Misclass LogLoss ASE 8.;\n  format Misclass LogLoss ASE 6.3;\n  length Role $10.;\n  Role = 'Training';\n  label Misclass = 'Misclassification Rate'\n        LogLoss = 'Log Loss'\n        ASE = 'Average Square Error';\n  Misclass = MiscTrain;\n  LogLoss = LogLossTrain;\n  ASE = ASETrain;\n  drop ASEOob ASETrain ASEValid\n       LogLossOob LogLossTrain LogLossValid  \n       MiscOob MiscTrain MiscValid ;      \nrun;\n\ndata forestVALIDfitstats;\n  set FORESTfitstatistics;\n  length Misclass LogLoss ASE 8.;\n  format Misclass LogLoss ASE 6.3;\n  length Role $10.;\n  Role = 'Validation';\n  label Misclass = 'Misclassification Rate'\n        LogLoss = 'Log Loss'\n        ASE = 'Average Square Error';\n  Misclass = MiscValid;\n  LogLoss = LogLossValid;\n  ASE = ASEValid;\n  drop ASEOob ASETrain ASEValid\n       LogLossOob LogLossTrain LogLossValid  \n       MiscOob MiscTrain MiscValid ;      \nrun;\n\ndata forestEVAL;\n  set forestTRAINfitstats forestVALIDfitstats;\nrun;\n\nproc sgplot data=forestEVAL;\n   styleattrs backcolor=lightgrey;\n   series x=Trees y=Misclass / \n          group=Role\n          smoothconnect\n          \n          dataskin=sheen;\n   yaxis label='Misclassification Rate';\n   xaxis label='Number of Trees';\n   title 'RANDOM FOREST: Misclassification Rate';\nrun;\n\nproc sgplot data=forestEVAL;\n   styleattrs backcolor=lightgrey;\n   series x=Trees y=LogLoss / \n          group=Role\n          smoothconnect\n          dataskin=sheen;\n   yaxis label='Log Loss Rate';\n   xaxis label='Number of Trees';\n   title 'RANDOM FOREST: Log Loss Rate';\nrun;\n\nproc sgplot data=forestEVAL;\n   styleattrs backcolor=lightgrey;\n   series x=Trees y=ASE / \n          group=Role\n          smoothconnect\n          dataskin=sheen;\n   yaxis label='Average Square Error';\n   xaxis label='Number of Trees';\n   title 'RANDOM FOREST: Average Square Error';\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"GRADIENT BOOSTING: Fit Statistics","outputs":[]},{"kind":2,"language":"sas","value":"data gboostTRAINfitstats;\n  set GBOOSTfitstatistics;\n  length Misclass LogLoss ASE 8.;\n  format Misclass LogLoss ASE 6.3;\n  length Role $10.;\n  Role = 'Training';\n  label Misclass = 'Misclassification Rate'\n        LogLoss = 'Log Loss'\n        ASE = 'Average Square Error';\n  Misclass = MiscTrain;\n  LogLoss = LogLossTrain;\n  ASE = ASETrain;\n  drop ASETrain ASEValid\n       LogLossTrain LogLossValid  \n       MiscTrain MiscValid ;      \nrun;\n\ndata gboostVALIDfitstats;\n  set GBOOSTfitstatistics;\n  length Misclass LogLoss ASE 8.;\n  format Misclass LogLoss ASE 6.3;\n  length Role $10.;\n  Role = 'Validation';\n  label Misclass = 'Misclassification Rate'\n        LogLoss = 'Log Loss'\n        ASE = 'Average Square Error';\n  Misclass = MiscValid;\n  LogLoss = LogLossValid;\n  ASE = ASEValid;\n  drop ASETrain ASEValid\n       LogLossTrain LogLossValid  \n       MiscTrain MiscValid ;      \nrun;\n\ndata gboostEVAL;\n  set gboostTRAINfitstats gboostVALIDfitstats;\nrun;\n\nproc sgplot data=gboostEVAL;\n   styleattrs backcolor=lightgrey;\n   series x=Trees y=Misclass / \n          group=Role\n          smoothconnect\n          \n          dataskin=sheen;\n   yaxis label='Misclassification Rate';\n   xaxis label='Number of Trees';\n   title 'GRADIENT BOOST: Misclassification Rate';\nrun;\n\nproc sgplot data=gboostEVAL;\n   styleattrs backcolor=lightgrey;\n   series x=Trees y=LogLoss / \n          group=Role\n          smoothconnect\n          dataskin=sheen;\n   yaxis label='Log Loss Rate';\n   xaxis label='Number of Trees';\n   title 'GRADIENT BOOST: Log Loss Rate';\nrun;\n\nproc sgplot data=gboostEVAL;\n   styleattrs backcolor=lightgrey;\n   series x=Trees y=ASE / \n          group=Role\n          smoothconnect\n          dataskin=sheen;\n   yaxis label='Average Square Error';\n   xaxis label='Number of Trees';\n   title 'GRADIENT BOOST: Average Square Error';\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"### **Step 6: SCORE the hold out sample**\n\nBy scoring the hold-out sample, we can obtain an unbiased assessment of a modelâ€™s performance on data that was not used during training. The hold-out sample acts as a proxy for real-world, unseen data, allowing data scientists to evaluate how well the model generalizes and predicts new observations. By scoring this sample, we can detect issues like overfitting, ensure the model's robustness, and gain confidence that it will perform well in production or on future datasets. This step is essential for validating the model's true effectiveness before deployment.","outputs":[]},{"kind":1,"language":"markdown","value":"RANDOM FOREST: use the ASTORE to score the hold out data ","outputs":[]},{"kind":2,"language":"sas","value":"title 'RANDOM FOREST: ASTORE Metadata';\nproc astore;\n    describe rstore=forestAstore;\n    score data=bankTEST rstore=forestAstore\n          out=forestPREDICTED copyvars=(status);\nrun;\ntitle;","outputs":[]},{"kind":1,"language":"markdown","value":"FOREST: Generate a frequency table for actual vs. predicted classes to create a Confusion Matrix","outputs":[]},{"kind":2,"language":"sas","value":"proc freq data=forestPREDICTED noprint;\n    tables status*I_Status / out=missclassFOREST;\nrun;\n\ndata missclassFOREST;\n    set missclassFOREST;\n    label = put(count, 8.);\nrun;\n\nproc sgplot data=missclassFOREST;\n    heatmap x=I_Status y=status / \n        colorresponse=count \n        colormodel=(lightgrey mediumgrey darkgoldenrod) \n        discretex discretey;\n    text x=I_Status y=status text=label / position=center;\n    xaxis label=\"Predicted Class\";\n    yaxis label=\"Actual Class\";\n    title \"RANDOM FOREST: Misclassification Table\";\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"GBOOST: use the ASTORE to score the hold out data ","outputs":[]},{"kind":2,"language":"sas","value":"title 'GRADIENT BOOST: ASTORE Metadata';\nproc astore;\n    describe rstore=gboostAstore;\n    score data=bankTEST rstore=gboostAstore\n          out=gboostPREDICTED copyvars=(status);\nrun;\ntitle;","outputs":[]},{"kind":1,"language":"markdown","value":"GBOOST: Generate a frequency table for actual vs. predicted classes to create a Confusion Matrix","outputs":[]},{"kind":2,"language":"sas","value":"proc freq data=gboostPREDICTED noprint;\n    tables status*I_Status / out=missclassGBOOST;\nrun;\n\ndata missclassGBOOST;\n    set missclassGBOOST;\n    label = put(count, 8.);\nrun;\n\nproc sgplot data=missclassGBOOST;\n    heatmap x=I_Status y=status / \n        colorresponse=count \n        colormodel=(lightgrey mediumgrey darkgoldenrod) \n        discretex discretey;\n    text x=I_Status y=status text=label / position=center;\n    xaxis label=\"Predicted Class\";\n    yaxis label=\"Actual Class\";\n    title \"GRADIENT BOOST: Misclassification Table\";\nrun;","outputs":[]}]